{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D_1HFdKxPNZ3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The purpose of this analysis was to build and optimize a neural network binary classification model to predict the success of nonprofit organizations funded by Alphabet Soup. The goal was to improve predictive performance to above 75% accuracy.\n",
        "\n",
        "Data Preprocessing\n",
        "\n",
        "* What variable(s) are the target(s) for your model? IS_SUCCESSFUL\n",
        "* What variable(s) are the features for your model? All Columns except IS_SUCCESSFUL,EIN, and NAME\n",
        "* What variable(s) should be removed from the input data because they are neither targets nor features? EIN, NAME\n",
        "\n",
        "\n",
        "Compiling, Training, and Evaluating the Model\n",
        "\n",
        "* How many neurons, layers, and activation functions did you select for your neural network model, and why?\n",
        "Layers and Neurons:\n",
        "\n",
        "The neural network model was designed with three total layers:\n",
        "\n",
        "Input Layer: Automatically sized based on the number of features (e.g., 43 features after encoding).\n",
        "\n",
        "First Hidden Layer: 80 neurons\n",
        "\n",
        "Second Hidden Layer: 30 neurons\n",
        "\n",
        "Output Layer: 1 neuron\n",
        "\n",
        "Activation Functions:\n",
        "\n",
        "Hidden Layers: Used the ReLU (Rectified Linear Unit) activation function.\n",
        "\n",
        "Why ReLU? It introduces non-linearity, helps avoid vanishing gradients, and performs well in most deep learning applications.\n",
        "\n",
        "Output Layer: Used the Sigmoid activation function.\n",
        "\n",
        "Why Sigmoid? It’s ideal for binary classification problems because it outputs a value between 0 and 1, representing the probability of the positive class (IS_SUCCESSFUL).\n",
        "\n",
        "Why These Choices?\n",
        "\n",
        "The 80–30 structure was selected based on a common heuristic: start with a number of neurons roughly equal to 2× the number of input features, then reduce in the next layer to condense learned patterns.\n",
        "\n",
        "The model was designed to be deep enough to capture complex relationships but shallow enough to prevent overfitting on a tabular dataset.\n",
        "\n",
        "ReLU and Sigmoid are standard and effective choices for deep learning models, especially for classification tasks like this.\n",
        "\n",
        "* Were you able to achieve the target model performance?\n",
        "\n",
        "The highest accuracy achieved was approximately 72%\n",
        "\n",
        "* What steps did you take in your attempts to increase model performance?\n",
        "Hyperparameter Tuning:\n",
        "\n",
        "Tried different combinations of neurons (e.g., 80-30, 100-50, 64-32).\n",
        "\n",
        "Changed the number of hidden layers (2 to 3 layers).\n",
        "\n",
        "Activation Functions:\n",
        "\n",
        "Tested tanh and LeakyReLU, but ReLU consistently performed better.\n",
        "\n",
        "Optimization:\n",
        "\n",
        "Experimented with different optimizers (adam, rmsprop).\n",
        "\n",
        "Adjusted learning rates and batch sizes.\n",
        "\n",
        "Epochs:\n",
        "\n",
        "Trained for 100 epochs with early stopping to avoid overfitting.\n",
        "\n",
        "Feature Engineering:\n",
        "\n",
        "Removed low-variance or redundant features.\n",
        "\n",
        "One-hot encoded categorical variables.\n",
        "\n",
        "Standardized numerical values (ASK_AMT, etc.).\n",
        "\n",
        "Regularization:\n",
        "\n",
        "Introduced dropout layers in some runs to reduce overfitting.\n",
        "\n",
        "Class Imbalance Handling:\n",
        "\n",
        "Applied class weights to balance the IS_SUCCESSFUL classes."
      ],
      "metadata": {
        "id": "mTtDl7zePOkm"
      }
    }
  ]
}